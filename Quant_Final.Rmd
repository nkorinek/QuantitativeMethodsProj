---
title: "Quantative Methods Final Project"
subtitle: "Forest Fire Time Series Data"
author: Nathan Korinek, Claire Simpson
output: 
  html_document:
    css: "lab.css"
---

```{r setup, include=FALSE}
# Setup the environment
library(knitr)
knitr::opts_chunk$set(fig.align='center',fig.width=10, fig.height=6, fig.path='Figs/',  warning=FALSE, echo=TRUE, eval=TRUE, message=FALSE)

r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
```

```{r, echo=T, eval=T, results='hide'}
library(maptools)
library(maps)
library(raster)
library(spatstat)
library(spdep)
library(ggmap)
library(ggsn)

# Load shape file
fire_data<-st_read("Quant_Fire_Data/Quant_Fire_Data.shp")

# Set the right projection information
fire_data<-st_set_crs(fire_data, 4326)

# Reproject the fire data to EPSG:26913, which represents UTM projection
projLocs<-st_transform(fire_data, CRS("+init=epsg:26913"))

# Load shape file
simple_fire_data<-st_read("Simplified_Quant_Fire_Data/Simplified_Quant_Fire_Data.shp")

# Set the right projection information
simple_fire_data<-st_set_crs(simple_fire_data, 4326)

# Reproject the fire data to EPSG:26913, which represents UTM projection
simple_projLocs<-st_transform(simple_fire_data, CRS("+init=epsg:26913"))
```



Clustering Analysis
```{r, echo=T, eval=T, results='hide'}
#kmeans
```

Add Columns for Training vs. Test vs Validation data
```{r, echo=T, eval=T, results='hide'}
Test_Ids = c("CO3741010757920121016","NM3700010423620110526","CO3894510543620120617","NM3692010445620110612","CO3747210346920110607","NM3696310515520100523")
Val_Ids = c('CO3726810830320120622', 'CO3935510767920100507', 'CO4005110538520100906', 'CO3943610521720120326')

fire_data['isTest'] <-'Train'
fire_data$isTest[fire_data$Event_ID %in% Test_Ids]<-'Test'
fire_data$isTest[fire_data$Event_ID %in% Val_Ids]<-'Validation'


```


Get attributes before and after fire to Split data into pre fire and post fire data
```{r, echo=T, eval=T, results='hide'}

# Convert fire_date to a Date object
fire_data$fire_date <- as.Date(paste(fire_data$Year, fire_data$Month, fire_data$Day, sep = "-"))

# Find the columns that occurred after the fire date
# after_fire_date_cols <- names(fire_data)[sapply(names(fire_data), function(col_name) {
#   col_date <- as.Date(paste0("01-", substring(col_name, 2)), format = "%d%b%Y")
#   col_date > fire_data$fire_date
# })]
my_df <- fire_data %>% st_drop_geometry()

# selected_names <- names(my_df)[8:799] #all names with var-month-year
# Vector of month names
month_names <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

#get new column names
attr_list <- list()
precip_list <- list()
temp_list <- list()
precip_list_post <- list()
temp_list_post <- list()
ndvi_list_post <- list()

# Loop through the desired number of attributes
for (i in 1:24) {
  #NOTE: 1 mo before fire = month in which fire occurred
  # Create the attribute name using paste0
  attr_name <- paste0("N", i, "before")
  # Add the attribute name to the list
  attr_list[i] <- attr_name
  precip_list[i] <- paste0("P", i, "before")
  temp_list[i] <- paste0("T", i, "before")
  
  #columns for after fire 
  precip_list_post[i] <- paste0("P", i, "after")
  temp_list_post[i] <- paste0("T", i, "after")
  ndvi_list_post[i] <- paste0("T", i, "after")
  
}

# Print the attribute names list
print(attr_list)

attr_list =unlist(attr_list)
precip_list = unlist(precip_list)
temp_list = unlist(temp_list)
precip_list_post = unlist(precip_list_post)
temp_list_post = unlist(temp_list_post)
ndvi_list_post = unlist(ndvi_list_post)


#iterate through rows and then cols to parse NDVI, preicp, temp for N months before/after fire
for (r in 1:nrow(my_df)){

  #get fire date
  this_fire_date = my_df$fire_date[r]
  fire_month <- as.numeric(format(this_fire_date, "%m"))
  fire_mo_name <- month_names[fire_month]
  fire_year <- format(this_fire_date, "%Y")
  
  #get index of column that represents month of fire 
  match_idx <- grep(paste('N',fire_mo_name, fire_year,sep=''), colnames(my_df))
  
  #get NDVI 24 months before the fire 
  col_indices <- seq(match_idx, match_idx - 24*3, by = -3)[1:24]
  for (i in seq_along(col_indices)){
    attr_name = attr_list[i]
    col_idx = col_indices[i]
    # print(c(colnames(my_df)[col_idx],attr_name))
    my_df[r, attr_name] <- my_df[r,col_idx]
  }
  
  #get NDVI 24 months AFTER the fire
  col_indices <- seq(match_idx+3, match_idx + 24*3, by = 3)#[1:24]
  for (i in seq_along(col_indices)){
    attr_name = ndvi_list_post[i]
    col_idx = col_indices[i]
    my_df[r, attr_name] <- my_df[r,col_idx]
  }
  
  match_idx = match_idx-1
  col_indices <- seq(match_idx, match_idx - 24*3, by = -3)[1:24]
  #get temp 24 months before fire
  for (i in seq_along(col_indices)){
    attr_name = temp_list[i]
    col_idx = col_indices[i]
    my_df[r, attr_name] <- my_df[r,col_idx]
  }
  #get temp 24 months AFTER the fire
  col_indices <- seq(match_idx+3, match_idx + 24*3, by = 3)
  for (i in seq_along(col_indices)){
    attr_name = temp_list_post[i]
    col_idx = col_indices[i]
    my_df[r, attr_name] <- my_df[r,col_idx]
  }
  
  #get precip 24 months before the fire
  match_idx = match_idx-1
  col_indices <- seq(match_idx, match_idx - 24*3, by = -3)[1:24]
  for (i in seq_along(col_indices)){
    attr_name = precip_list[i]
    col_idx = col_indices[i]
    my_df[r, attr_name] <- my_df[r,col_idx]
  }
  
  #get precip 24 months AFTER the fire 
  col_indices <- seq(match_idx+3, match_idx + 24*3, by = 3)
  for (i in seq_along(col_indices)){
    attr_name = precip_list_post[i]
    col_idx = col_indices[i]
    my_df[r, attr_name] <- my_df[r,col_idx]
  }
  
}
```
  
Normalize Data
```{r, echo=T, eval=T, results='hide'}

# Select columns to normalize
my_df['id'] <- seq(1:nrow(my_df))
dont_scale <- c("id","Event_ID", "Month","Day","Year","fire_date","isTest")
cols_to_normalize <- dplyr::setdiff(names(my_df), dont_scale)

# Normalize selected columns
df_scaled <- my_df %>% 
  dplyr::select(cols_to_normalize) %>% 
  dplyr::mutate_if(is.numeric, scale) %>%
  cbind(my_df[dont_scale])#, my_df[dont_scale])

```

Apply Seasonal Differencing to De-trend data/ control periodicity!
```{r, echo=T, eval=T, results='hide'}

# calculate the seasonal mean per point

# Identify columns with "N" values for each month using regular expressions
n_cols <- grep("^N[A-Za-z]{3}\\d{4}$", names(my_df))

# Create a new data frame with the monthly means for each row
#old:as.data.frame(rowMeans(x[grepl("^NJan\\d{4}$", names(x))],na.rm=TRUE)),
monthly_means <- data.frame(t(apply(my_df[n_cols], 1, function(x) {
  # x <- as.numeric(x)
  cbind(mean(x[grepl("^NJan\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NFeb\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NMar\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NApr\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NMay\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NJun\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NJul\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NAug\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NSep\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NOct\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NNov\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^NDec\\d{4}$", names(x))],na.rm=TRUE)
  )
})))

# Rename columns with the month names
colnames(monthly_means) <- paste0("N", month.abb, "Mean")
Ncols <- colnames(monthly_means)

# Add any non-"N" columns to the new data frame
other_cols <- setdiff(names(monthly_means),names(my_df))
# monthly_means[other_cols] <- my_df[other_cols]

my_df[other_cols] <- monthly_means 

## same for precip
n_cols <- grep("^P[A-Za-z]{3}\\d{4}$", names(my_df))

# Create a new data frame with the monthly means for each row
#old:as.data.frame(rowMeans(x[grepl("^NJan\\d{4}$", names(x))],na.rm=TRUE)),
monthly_means <- data.frame(t(apply(my_df[n_cols], 1, function(x) {
  # x <- as.numeric(x)
  cbind(mean(x[grepl("^PJan\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PFeb\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PMar\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PApr\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PMay\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PJun\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PJul\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PAug\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PSep\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^POct\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PNov\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^PDec\\d{4}$", names(x))],na.rm=TRUE)
  )
})))

# Rename columns with the month names
colnames(monthly_means) <- paste0("P", month.abb, "Mean")
Pcols <- colnames(monthly_means)

# Add any non-"N" columns to the new data frame
other_cols <- setdiff(names(monthly_means),names(my_df))
# monthly_means[other_cols] <- my_df[other_cols]

my_df[other_cols] <- monthly_means 

## same for TEMPERATURE
n_cols <- grep("^T[A-Za-z]{3}\\d{4}$", names(my_df))

# Create a new data frame with the monthly means for each row
monthly_means <- data.frame(t(apply(my_df[n_cols], 1, function(x) {
  # x <- as.numeric(x)
  cbind(mean(x[grepl("^TJan\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TFeb\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TMar\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TApr\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TMay\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TJun\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TJul\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TAug\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TSep\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TOct\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TNov\\d{4}$", names(x))],na.rm=TRUE),
    mean(x[grepl("^TDec\\d{4}$", names(x))],na.rm=TRUE)
  )
})))

# Rename columns with the month names
colnames(monthly_means) <- paste0("T", month.abb, "Mean")
Tcols <- colnames(monthly_means)


# Add any non-"N" columns to the new data frame
other_cols <- setdiff(names(monthly_means),names(my_df))
# monthly_means[other_cols] <- my_df[other_cols]

my_df[other_cols] <- monthly_means 


final_df <- my_df

### "^T[A-Za-z]{3}\\d{4}$"
all_month_cols <- c(Ncols,Pcols,Tcols)
for (month_col in all_month_cols) { #iterate through all variable mean monthly columns (36)
  my_exp <- paste0("^",substr(month_col, 1, 1), substr(month_col, 2, 4), "\\d{4}$")
  my_df[, grepl(my_exp, names(my_df))] <- 
    my_df[, grepl(my_exp, names(my_df))] - my_df[, month_col]
}

```

Here: ACTUALLY split data into training and test :)
```{r, echo=T, eval=T, results='hide'}

#split into train/test sets (val is part of train right now)
train_Xy <- my_df %>% dplyr::filter(my_df$isTest != 'Test') #currently this keeps validation rows in training set
# train_y <- my_df %>% dplyr::filter(my_df$isTest != 'Test')
test_Xy <- my_df %>% dplyr::filter(my_df$isTest == 'Test')
# test_y <- my_df %>% dplyr::filter(my_df$isTest == 'Test')


# Create input and output data frames
static_list <- c('slope','chili','elevation','aspect','mtpi')#colnames(my_df)[1:6]
predictor_cols <- c(attr_list,precip_list, temp_list, static_list) #X (predictor) column names
target_cols <- c(precip_list_post, temp_list_post, ndvi_list_post) #y (target) col names
# all_cols <- c(target_cols,predictor_cols)
# train_Xy <- train_Xy[all_cols]
# test_Xy <- test_Xy[c(predictor_cols,target_cols)]
# train_y <- test_y[target_cols]
# test_y <- test_y[target_cols]


```

Random Forests Analysis

We will fire run recursive feature elimination to get a sense of which predictor
variables are the most important. We first need to prepare the data for input, 
including transforming the data to remove the seasonal fluctuations (make it
stationary): 

```{r, echo=T, eval=T, results='hide'}
#random forests
library(randomForest) 
library(tidyr)
library(sf)
library(caret)
library(doParallel)
# install.pacakges("rlang")#needed to fix a caret dependency/version issue
library(caret)

# if installing caret doesnt work try:
# library(devtools)
# devtools::install_url("https://cran.r-project.org/src/contrib/caret_6.0-78.tar.gz")

#subset out columns from dataset (remove non-predictors/targets)
# names <- names(fire_data)
# to_remove <- c("Event_ID", "geometry", "index_righ")
# keep_names <- names[! names %in% to_remove]
# keep_names
# fire_data_4_model <- fire_data[keep_names]
# fire_data_4_model <- fire_data_4_model %>% st_drop_geometry()

#check for complete cases (will remove everything bc some columns are blank...)
# fire_data_4_model <- fire_data_4_model[complete.cases(fire_data_4_model), ]

#drop row where column 'elevation' is NA
train_Xy<- train_Xy %>% tidyr::drop_na(elevation)
test_Xy<- test_Xy %>% tidyr::drop_na(elevation)

#drop any column where there is an NA value
# train_X<-train_X[ , apply(train_X, 2, function(x) !any(is.na(x)))]

#TEMPORARY until we do interpolation to fill missing values
train_Xy[is.na(train_Xy)] <- 0
test_Xy[is.na(test_Xy)] <- 0


#Run Recursive Feature Elimination

#RFE parameters
subsets <- seq(20, 700, by=20)#c(1:(length(fire_data_4_model)-1))
# seeds <- vector(mode = "list", length = 51)
seeds <- vector(mode = "list", length = 35)
for(i in 1:75) seeds[[i]] <- sample.int(1000, length(subsets) + 1)
seeds[[76]] <- sample.int(1000, 1)

ctrl.RFE <- caret::rfeControl(functions = rfFuncs,
                       method = "repeatedcv",
                       number = 15,
                       repeats = 5,
                       seeds = seeds, 
                       verbose = FALSE)

#this code makes it run in parallel
c1 <- makeCluster(detectCores()-1)
registerDoParallel(c1)
set.seed(9)
target <- c('NApr2021')
rf.RFE <- rfe(x = fire_data_4_model[! fire_data_4_model %in% target],
              y = fire_data_4_model$NApr2021,
              sizes = subsets,
              # na.rm=TRUE,
              rfeControl = ctrl.RFE,
              allowParallel = TRUE
)
stopCluster(c1)              

gc()

# Look at the results
rf.RFE

rf.RFE$fit

rf.RFE$results

plot(rf.RFE) # default plot is for Accuracy, but it can also be changed to Kappa
plot(rf.RFE, metric="Kappa", main='RFE Kappa')
plot(rf.RFE, metric="Accuracy", main='RFE Accuracy')

```

We now can run the Random Forests model.

Options for RF strategies:
1.) create n RF models, one for each timestep we want to predict. Say, 12 different RF models, 
1 for each of the 12 months in the second year after the fire. We fire need to create 
12 training and test sets, where the training data consists of the 12 months immediately 
preeceeding the fire

2.) Create 1 RF model where there is an input predictor variable (feature) that 
tells the model which timestep after the fire the model is supposed to predict

```{r, echo=T, eval=T, results='hide'}
#Run Option 1:

#store RMSE of test set in this dataframe:
rmse_df <- data.frame()
r2_df <- data.frame()
for (i in 1:3){
  print(i)
  #create training set with t timesteps pre-prediction
  #e.g. first want to predict NDVI for month after fire, using 24-months pre-fire 
  
  #get list of attributes to keep as predictors:
  end <- length(attr_list)
  keep_attr_pre <- c(attr_list[i:end], precip_list[i:end], temp_list[i:end])
  len <- length(attr_list) - length(attr_list[i:end]) 
  #^ how many timesteps do we need in order to ensure that we have 24 pre-prediction inputs?
  if (len>0){
    keep_attr_post <- c(ndvi_list_post[1:len], precip_list_post[1:len], temp_list_post[1:len])
  }else{
    keep_attr_post = c()
  }
   #get attribute to kee as target
  target_col <- ndvi_list_post[i]
  print(c("target:", target_col))
  
  keep <- c(keep_attr_pre, keep_attr_post, static_list,target_col)
  
  #split data into input and target/output
  current <- train_Xy[keep]
  current_test <- test_Xy[keep]
  #current_y <- train_Xy[keep_y]
  
  #run RF!!
  # model.rf <- randomforest()
  #############################
  
  #subsets max number should be the same or equal to the number of covariates being used
  # determine number of covariates
  subsets <- length(keep)

  # set seeds to get reproducable results when running the process in parallel
  set.seed(12)
  seeds <- vector(mode = "list", length=76)
  for(j in 1:75) seeds[[j]] <- sample.int(1000, length(1:subsets) + 1)
  seeds[[76]] <- sample.int(1000, 1)
  
  # set up the train control
  # fitControl <- trainControl(method = "none",#repeatedcv", 
  #                            # number = 5,#15
  #                            # repeats = 1, #change to 3+
  #                            # p = 0.7, #30% used for test set, 70% used for training set
  #                            # selectionFunction = 'best', 
  #                            # classProbs = T,
  #                            savePredictions = T, 
  #                            # returnResamp = 'final',
  #                            search = "random",
  #                            verboseIter = TRUE,
  #                            seeds = seeds)
  # 
  # # 5.1  Random Forest - Parallel process, highlight and run from c1 to stopCluster(c1)
  # c1 <- makeCluster(detectCores()-1)
  # registerDoParallel(c1)
  # set.seed(48)
  # #NOTE: need to edit T1after
  # rfFit = caret::train(T1after ~ ., data = current,
  #               method="rf", 
  #               trControl = fitControl, 
  #               ntree = 500, #number of trees default is 500, which seems to work best anyway. 
  #               tuneLength=10,
  #               metric = 'RMSE', 
  #               na.action=na.pass,
  #               keep.forest=TRUE, # added this line and the next for partial dependence plots
  #               importance=TRUE)
  # 
  # stopCluster(c1)
  # 
  # gc()
  # 
  # # Inspect rfFit
  # rfFit$finalModel
  # 
  # #look at overall accuracy, kappa, and SD associated with each mtry value
  # print(rfFit)
  # rfFit$results
  
  
  # Convert caret wrapper into randomforest model object for raster prediction
  # rf.model <- rfFit$finalModel
  # rf.pred <- predict(rf_model$finalModel, newdata=current_test)
  
  
  # varImpPlot(rf.model)
  
  
  ##---------------------- different RF function ---------------------
  # Create the formula for randomForest
formula <- as.formula(paste(target_col, paste(names(current)[!names(current) %in% target_col], collapse = "+"), sep = "~"))
  
  # Fit the random forest model
  rf_model <- randomForest(formula=formula, data = current, importance = TRUE, ntree=500,mtry=50,type='regression')

  
  
  # Make predictions on the test set
  rf_pred <- predict(rf_model, newdata = current_test)
  
  # Check the accuracy of the model
  # table(rf_pred, current_test$T1after)
  varImpPlot(rf_model)

  test_rmse <- sqrt(mean((current_test$T1after - rf_pred)^2))
  print(c("RMSE on test set:", test_rmse))
  # rmse_df[target_col] <- test_rmse 
  rmse_df[1, target_col] <- test_rmse

  
  #r2
  test_r2 <- 1 - sum((current_test$T1after - rf_pred)^2) / sum((current_test$T1after - mean(current_test$T1after))^2)
  print(c("test R2:",test_r2))
  r2_df[1, target_col] <- test_r2

  
  plot(current_test$T1after,rf_pred, xlim=c(40,160), ylim=c(40,160),asp=1)
  
  test_Xy[paste("pred_",target_col,sep='')] = rf_pred 

  
}




```
